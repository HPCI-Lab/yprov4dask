from dask.order import order
from dask.task_spec import DataNode, Task, TaskRef, Alias
from dask.typing import Key
from distributed.diagnostics.plugin import SchedulerPlugin
from distributed.scheduler import Scheduler, TaskStateState as SchedulerTaskState
from prov_tracking.documenter import Documenter
from prov_tracking.utils import RunnableTaskInfo

import datetime as dt
from typing import Any, cast
from traceback import format_exc

class ProvTracker(SchedulerPlugin):
  """Provenance tracking plugin"""

  def __init__(self, **kwargs):
    """Additional arguments:
    - `keep_traceback: bool`: tells if the plugin should register the traceback
    of the exceptions generated by failed tasks. Defaults to `False`.
    You can also provide any additional argument accepted by
    `pro_tracking.Documenter`.
    """

    # kwargs that are only used by the plugin, should be removed because some
    # methods used by libraries, unpack kwargs and any additional argument would
    # cause an exception
    self.keep_traceback: bool = kwargs.pop('keep_traceback', False)
    self.documenter = Documenter(**kwargs)
    self.kwargs: dict[str, Any] = kwargs
    self.closed = False
    # Used to avoid registering multiple times the same task. A task can be put
    # multiple times in waiting state
    self.registered_tasks: set[Key] = set()
    # Info about all tasks encountered
    self.all_tasks: dict[Key, Task | DataNode] = {}
    self.all_runnables: dict[Key, RunnableTaskInfo] = {}
    # Tasks executed within _execute_subgraph are saved here under the key of
    # the task that executes _execute_subgraph. That task info are not actually
    # saved, jusst its children.
    self.macro_tasks: dict[Key, list[Key]] = {}

  def start(self, scheduler: Scheduler):
    self._scheduler = scheduler

  def transition(
    self, key: Key, start: SchedulerTaskState, finish: SchedulerTaskState,
    *args, **kwargs
  ):
    try:
      task = self._scheduler.tasks[key]

      # Here tasks are seen in reverse dependency order, i.e. tasks with no
      # dependencies are seen before tasks which depend on them
      if start == 'waiting' and key not in self.registered_tasks:
        self.registered_tasks.add(key)
        if isinstance(task.run_spec, DataNode):
          self.documenter.register_non_runnable_task(str(key), task.run_spec)
          self.all_tasks[key] = task.run_spec
        elif isinstance(task.run_spec, Task):
          if not ProvTracker._is_dask_internal(task.run_spec):
            # Here we pass all_tasks as internal_deps because the task might
            # contain some TaskRef to another task, so we need a way to follow
            # the ref.
            self.all_runnables[key] = RunnableTaskInfo(
              task, internal_deps=self.all_tasks
            )
            self.all_tasks[key] = task.run_spec
            self.macro_tasks[key] = [key]
          else:
            infos = self._track_dask_internal(task.run_spec, task.group_key)
            self.all_runnables.update(infos)
            self.macro_tasks[key] = [sub_key for sub_key in infos]
        else:
          # This task is an alias for another task that calls _execute_subgraph.
          # It should be safe to ignore it, as an actual task with the same key
          # should always exist within the _execute_subgraph call
          pass

      elif start == 'processing' and key in self.macro_tasks:
        now = dt.datetime.now()
        for sub_key in self.macro_tasks[key]:
          info = self.all_runnables[sub_key]
          info.start_time = now

      elif start == 'memory' and key in self.macro_tasks:
        now = dt.datetime.now()
        for sub_key in self.macro_tasks[key][:-1]:
          info = self.all_runnables[sub_key]
          info.finish_time = now
          self.documenter.register_successful_task(info, None, None)
        dtype = task.type
        nbytes = task.nbytes
        info = self.all_runnables[self.macro_tasks[key][-1]]
        info.finish_time = now
        self.documenter.register_successful_task(info, dtype, nbytes)
        self.macro_tasks.pop(key) # Avoid registering two times the same activity

      elif start == 'erred' and key in self.macro_tasks:
        # It's impossible to detect what task has failed, so if this macro task
        # has many sub-tasks, all are represented as erroneous in the provenance
        # document

        now = dt.datetime.now()
        for sub_key in self.macro_tasks[key][:-1]:
          info = self.all_runnables[sub_key]
          info.finish_time = now
          self.documenter.register_failed_task(info, None, None, None)
        # The task is finished with an error, so register the exception
        info = self.all_runnables[self.macro_tasks[key][-1]]
        info.finish_time = now
        text = task.exception_text or ''
        blamed_task = task.exception_blame
        traceback = None
        if self.keep_traceback:
          traceback = task.traceback_text
        self.documenter.register_failed_task(info, text, traceback, blamed_task)
        self.macro_tasks.pop(key)

        # When an exception occurs, the plugin is closed before it has the chance
        # to detect the erred task and register its information. So, if the plugin
        # has already been closed, serialize the document again.
        if self.closed:
          self.documenter.serialize()
    except Exception:
      print(f'Task {key} generated an exception:\n{format_exc()}')

  async def close(self):
    self.closed = True

    try:
      self.documenter.serialize()
    except Exception as e:
      print(f'Close: {e}')

  @staticmethod
  def _is_dask_internal(run_spec: Task) -> bool:
    """Checks if the given runnable task is a call to a dask internal function."""

    func = run_spec.func
    if hasattr(func, '__name__'):
      return (
        func.__name__ == '_execute_subgraph' and
        func.__module__ == 'dask._task_spec'
      )
    return False

  @staticmethod
  def _get_key(parent: Key, child: Key) -> Key:
    """Takes the key of a task `child` which has been started by `parent`, i.e.
    child is part of the `inner_dsk` dictionary of `parent.args`. If `child` is
    a non-unique key, returns a new key that embeds `parent`'s info to produce
    a new unique key."""

    key = child
    # This first condition should always be True
    if isinstance(parent, tuple) and len(parent) > 1:
      if isinstance(child, tuple):
        if len(child) == 1:
          key = (child[0], *parent[1:])
        #else:
        #  key = (child[0], *parent[1:], *child[1:])
      elif not isinstance(child, tuple):
        key = (child, *parent[1:])
      # Otherwise, the child key is fine as it is
    return key

  def _track_dask_internal(
    self, run_spec: Task, group_key: str
  ) -> dict[Key, RunnableTaskInfo]:
    """Given a task that executes `dask._task_spec._execute_subgraph` follows
    the subgraph associated to that call and records all the information about
    tasks in that subgraph. `self.all_tasks` is kept updated. Returns a
    dictionary with the info of all analysed tasks.
    
    #### Note
    This call is recursive, but this should be safe as the maximum recursion
    level is (apparently) at most 1.
    """

    # These are all tasks that will be executed by _execute_subgraph
    inner_dsk = cast(dict[Key, Task | Alias | DataNode], run_spec.args[0])
    # Some dependencies are referenced with names in inkeys that might be
    # different from names already used before for the same resource
    inkeys = cast(tuple[Key, ...], run_spec.args[2])
    dependencies = cast(tuple[TaskRef | DataNode, ...], run_spec.args[3:])
    # Keeps the association between a dependency and its inkey. This dict always
    # used the original key for a dependency, even if its non-unique
    internal_deps: dict[Key, Any] = {}
    # Tasks in inner_dsk might be using non-unique keys. This dictionary keeps
    # the association between the original non-unique key of a task and a new
    # unique key used later on when the task info is serialized
    unique_keys: dict[Key, Key] = {}

    for key, dep in zip(inkeys, dependencies):
      if isinstance(dep, TaskRef):
        # Refs are always for already seen tasks, so this is safe. Also, tasks
        # whose key has been changed because it was non-unique, are never seen
        # here
        internal_deps[key] = self.all_tasks[dep.key]
      else:
        # As this value is actually ready, and doesn't come from any other
        # recognizable task, just take the raw value
        internal_deps[key] = dep.value
    
    priorities = order(inner_dsk)
    infos: dict[Key, RunnableTaskInfo] = {}
    for key, node in sorted(inner_dsk.items(), key=lambda it: priorities[it[0]]):
      # node is an alias or a task executed by a worker
      if isinstance(node, Task):
        unique_key = ProvTracker._get_key(run_spec.key, key)
        unique_keys[key] = unique_key
        self.all_tasks[unique_key] = node
        if ProvTracker._is_dask_internal(node):
          # That executing _execute_subgraph, are not saved in the prov document
          infos.update(self._track_dask_internal(node, group_key))
        else:
          node_deps = []
          for dep in cast(set[Key], node.dependencies):
            if dep in internal_deps:
              node_deps.append(internal_deps[dep])
            else:
              unique_dep_key = unique_keys.get(dep, dep)
              node_deps.append(self.all_tasks[unique_dep_key])
          infos[unique_key] = RunnableTaskInfo(
            specs=node, group_key=group_key, dependencies=node_deps,
            internal_deps=internal_deps, unique_keys=unique_keys
          )
      elif isinstance(node, Alias):
        unique_key = ProvTracker._get_key(run_spec.key, key)
        unique_target = unique_keys.get(node.target, node.target)
        self.all_tasks[unique_key] = self.all_tasks[unique_target]
      else:
        # node is a DataNode
        if node.key not in self.all_tasks:
          self.documenter.register_non_runnable_task(node.key, node)
          self.all_tasks[node.key] = node
        else:
          # Currently this branch has never been executed, so it might be
          # safe to assume DataNodes seen here have a unique key
          print('duplicate')
    
    return infos