from dask.order import order
from dask.task_spec import DataNode, Task, TaskRef, Alias
from dask.typing import Key
from distributed.diagnostics.plugin import SchedulerPlugin
from distributed.scheduler import Scheduler, TaskStateState as SchedulerTaskState
from prov_tracking.documenter import Documenter
from prov_tracking.utils import RunnableTaskInfo

import datetime as dt
from typing import Any, cast
from traceback import format_exc

class ProvTracker(SchedulerPlugin):
  """Provenance tracking plugin"""

  def __init__(self, **kwargs):
    """Additional arguments:
    - `keep_traceback: bool`: tells if the plugin should register the traceback
    of the exceptions generated by failed tasks. Defaults to `False`.
    You can also provide any additional argument accepted by
    `pro_tracking.Documenter`.
    """

    # kwargs that are only used by the plugin, should be removed because some
    # methods used by libraries, unpack kwargs and any additional argument would
    # cause an exception
    self.keep_traceback: bool = kwargs.pop('keep_traceback', False)
    self.documenter = Documenter(__name__)
    self.kwargs: dict[str, Any] = kwargs
    self.closed = False
    # Used to avoid registering multiple times the same task. A task can be put
    # multiple times in waiting state
    self.registered_tasks: set[Key] = set()
    # Info about all tasks encountered
    self.all_tasks: dict[Key, Task | DataNode] = {}
    self.all_runnables: dict[Key, RunnableTaskInfo] = {}
    # Tasks executed within _execute_subgraph are saved here under the key of
    # the task that executes _execute_subgraph. That task info are not actually
    # saved, jusst its children.
    self.macro_tasks: dict[Key, list[Key]] = {}

  def start(self, scheduler: Scheduler):
    self._scheduler = scheduler

  def transition(
    self, key: Key, start: SchedulerTaskState, finish: SchedulerTaskState,
    *args, **kwargs
  ):
    try:
      task = self._scheduler.tasks[key]

      # Here tasks are seen in reverse dependency order, i.e. tasks with no
      # dependencies are seen before tasks which depend on them
      if start == 'waiting' and key not in self.registered_tasks:
        self.registered_tasks.add(key)
        if isinstance(task.run_spec, DataNode):
          self.all_tasks[key] = task.run_spec
          self.documenter.register_data(task.run_spec)
        elif isinstance(task.run_spec, Task):
          if not ProvTracker._is_dask_internal(task.run_spec):
            info = RunnableTaskInfo(
              task, all_tasks=self.all_tasks
            )
            self.all_runnables[key] = info
            self.all_tasks[key] = task.run_spec
            self.macro_tasks[key] = [key]
            self.documenter.register_task(info)
          else:
            infos = self._track_dask_internal(task.run_spec, task.group_key)
            self.all_runnables.update(infos)
            self.macro_tasks[key] = []
            for sub_key, info in infos.items():
              self.macro_tasks[key].append(sub_key)
              self.documenter.register_task(info)
        else:
          target = cast(Alias, task.run_spec).target
          self.all_tasks[key] = self.all_tasks[target]

      elif start == 'processing' and key in self.macro_tasks:
        now = dt.datetime.now()
        for sub_key in self.macro_tasks[key]:
          info = self.all_runnables[sub_key]
          info.start_time = now

      elif start == 'memory' and key in self.macro_tasks:
        now = dt.datetime.now()
        for sub_key in self.macro_tasks[key][:-1]:
          info = self.all_runnables[sub_key]
          info.finish_time = now
          self.documenter.register_task_success(info, None, None)
        dtype = task.type
        nbytes = task.nbytes
        info = self.all_runnables[self.macro_tasks[key][-1]]
        info.finish_time = now
        self.documenter.register_task_success(info, dtype, nbytes)
        self.macro_tasks.pop(key) # Avoid registering two times the same activity

      elif start == 'erred' and key in self.macro_tasks:
        # It's impossible to detect what task has failed, so if this macro task
        # has many sub-tasks, all are represented as erroneous in the provenance
        # document

        now = dt.datetime.now()
        for sub_key in self.macro_tasks[key][:-1]:
          info = self.all_runnables[sub_key]
          info.finish_time = now
          self.documenter.register_task_failure(info, None, None, None)
        # The task is finished with an error, so register the exception
        info = self.all_runnables[self.macro_tasks[key][-1]]
        info.finish_time = now
        text = task.exception_text or ''
        blamed_task = task.exception_blame
        traceback = None
        if self.keep_traceback:
          traceback = task.traceback_text
        self.documenter.register_task_failure(info, text, traceback, blamed_task)
        self.macro_tasks.pop(key)

        # When an exception occurs, the plugin is closed before it has the chance
        # to detect the erred task and register its information. So, if the plugin
        # has already been closed, serialize the document again.
        if self.closed:
          self.documenter.serialize()
    except Exception:
      print(f'Task {key} generated an exception:\n{format_exc()}')

  async def close(self):
    self.closed = True
    self.documenter.terminate()

    try:
      self.documenter.serialize()
    except Exception as e:
      print(f'Close: {e}')

  @staticmethod
  def _is_dask_internal(run_spec: Task) -> bool:
    """Checks if the given runnable task is a call to a dask internal function."""

    func = run_spec.func
    if hasattr(func, '__name__'):
      return (
        func.__name__ == '_execute_subgraph' and
        func.__module__ == 'dask._task_spec'
      )
    return False

  @staticmethod
  def _make_unique_key(parent: Key, child: Key) -> Key:
    """Takes the key of a task `child` which has been started by `parent`, i.e.
    child is part of the `inner_dsk` dictionary of `parent.args`. If `child` is
    a non-unique key, returns a new key that embeds `parent`'s info to produce
    a new unique key."""

    key = child
    # This first condition should always be True
    if isinstance(parent, tuple) and len(parent) > 1:
      if isinstance(child, tuple) and len(child) == 1:
        key = (child[0], *parent[1:])
      elif not isinstance(child, tuple):
        key = (child, *parent[1:])
      # Otherwise, the child key is fine as it is
    return key

  def _track_dask_internal(
    self, run_spec: Task, group_key: str, unique_keys: dict[Key, Key] = {}
  ) -> dict[Key, RunnableTaskInfo]:
    """Given a task that executes `dask._task_spec._execute_subgraph` follows
    the subgraph associated to that call and records all the information about
    tasks in that subgraph. `self.all_tasks` is kept updated. Returns a
    dictionary with the info of all analysed tasks.
    
    #### Note
    This call is recursive, but this should be safe as the maximum recursion
    level is (apparently) at most 1.
    """

    # These are all tasks that will be executed by _execute_subgraph
    inner_dsk = cast(dict[Key, Task | Alias | DataNode], run_spec.args[0])

    # Some dependencies are referenced with names in inkeys that might be
    # different from names already used before for the same resource
    inkeys = cast(tuple[Key, ...], run_spec.args[2])
    dependencies = cast(tuple[TaskRef | DataNode, ...], run_spec.args[3:])
    internal_deps: dict[Key, Any] = {}
    for key, dep_key in zip(inkeys, dependencies):
      if isinstance(dep_key, TaskRef):
        # Refs are always for already-seen tasks, so this is safe. Also, tasks
        # whose key has been changed because it was non-unique, are never seen
        # here
        internal_deps[key] = self.all_tasks[dep_key.key]
      else:
        # As this value is actually ready and doesn't come from any other
        # recognizable task, just take the raw value
        internal_deps[key] = dep_key.value
    
    priorities = order(inner_dsk)
    infos: dict[Key, RunnableTaskInfo] = {}
    for key, node in sorted(inner_dsk.items(), key=lambda it: priorities[it[0]]):
      if isinstance(node, Task):
        unique_key = ProvTracker._make_unique_key(run_spec.key, key)
        unique_keys[key] = unique_key
        self.all_tasks[unique_key] = node
        if ProvTracker._is_dask_internal(node):
          infos.update(self._track_dask_internal(node, group_key, unique_keys))
        else:
          node_deps = {}
          for dep_key in cast(set[Key], node.dependencies):
            if dep_key in internal_deps:
              node_deps[dep_key] = internal_deps[dep_key]
            else:
              unique_dep_key = unique_keys.get(dep_key, dep_key)
              node_deps[dep_key] = self.all_tasks[unique_dep_key]
          infos[unique_key] = RunnableTaskInfo(
            specs=node, group_key=group_key, dependencies=node_deps,
            all_tasks=self.all_tasks, unique_keys=unique_keys
          )
      elif isinstance(node, Alias):
        unique_key = ProvTracker._make_unique_key(run_spec.key, key)
        unique_target = unique_keys.get(node.target, node.target)
        self.all_tasks[unique_key] = self.all_tasks[unique_target]
      else:
        self.documenter.register_data(node)
        self.all_tasks[node.key] = node
    
    # Execute subgraph returns the output of task `outkey`, so set this task is
    # treated as an alias for `outkey`
    outkey: Key = run_spec.args[1]
    outkey = unique_keys.get(outkey, outkey)
    self.all_tasks[run_spec.key] = self.all_tasks[outkey]
    return infos