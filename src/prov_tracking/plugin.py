from logging import info
from dask.order import order
from dask.task_spec import DataNode, Task, TaskRef, Alias
from dask.typing import Key
from distributed.diagnostics.plugin import SchedulerPlugin
from distributed.scheduler import Scheduler, TaskState, TaskStateState as SchedulerTaskState
from prov_tracking.documenter import Documenter
from prov_tracking.utils import RunnableTaskInfo, make_unique_key

import datetime as dt
from typing import Any, cast
from traceback import format_exc

class ProvTracker(SchedulerPlugin):
  """Provenance tracking plugin"""

  def __init__(self, **kwargs):
    """Additional arguments:
    - `name: str`: name of the workflow. Defaults to `__name__`.
    - `keep_traceback: bool`: tells if the plugin should register the traceback
    of the exceptions generated by failed tasks. Defaults to `False`.
    You can also provide any additional argument accepted by
    `pro_tracking.Documenter`.
    """

    name = kwargs.pop('name', __name__)
    self.keep_traceback: bool = kwargs.pop('keep_traceback', False)
    self.documenter = Documenter(name, **kwargs)
    self.closed = False
    # Used to avoid registering multiple times the same task. A task can be put
    # multiple times in waiting state
    self.registered_tasks: set[Key] = set()
    # Info about all tasks encountered
    self.all_tasks: dict[Key, Task | DataNode] = {}
    self.all_runnables: dict[Key, RunnableTaskInfo] = {}
    # Tasks executed within _execute_subgraph are saved here under the key of
    # the task that executes _execute_subgraph. That task info are not actually
    # saved, jusst its children.
    self.macro_tasks: dict[Key, list[Key]] = {}
    self.erred_tasks: dict[Key, Task | DataNode] = {}

  def start(self, scheduler: Scheduler):
    self._scheduler = scheduler

  def transition(
    self, key: Key, start: SchedulerTaskState, finish: SchedulerTaskState,
    *args, **kwargs
  ):
    try:
      task = self._scheduler.tasks[key]

      # Here tasks are seen in reverse dependency order, i.e. tasks with no
      # dependencies are seen before tasks which depend on them
      if start == 'waiting' and key not in self.registered_tasks:
        self.registered_tasks.add(key)
        if isinstance(task.run_spec, DataNode):
          self.all_tasks[key] = task.run_spec
          self.documenter.register_data(task.run_spec)
        elif isinstance(task.run_spec, Task):
          if str(key).startswith(('rechunk-merge-rechunk-split-store-map', '(\'rechunk-merge-rechunk-split-store-map', 'finalize', '(\'store-map')):
            pass
          self.macro_tasks[key] = []
          infos = None
          if not ProvTracker._is_dask_internal(task.run_spec):
            self.all_tasks[key] = task.run_spec
            self.macro_tasks[key].append(key)
            infos = self._track_task(task)
          else:
            infos = self._track_dask_internal(
              task.run_spec, task.group_key, task.key, sub_tasks={}
            )
          self.all_runnables.update(infos)
          for sub_key, info in infos.items():
            self.macro_tasks[key].append(sub_key)
            self.documenter.register_task(info)
        else:
          target = cast(Alias, task.run_spec).target
          self.all_tasks[key] = self.all_tasks[target]

      elif start == 'processing' and key in self.macro_tasks:
        now = dt.datetime.now()
        for sub_key in self.macro_tasks[key]:
          info = self.all_runnables[sub_key]
          info.start_time = now
          self.documenter.register_task_dependencies(info)

      elif start == 'memory' and key in self.macro_tasks:
        now = dt.datetime.now()
        for sub_key in self.macro_tasks[key][:-1]:
          info = self.all_runnables[sub_key]
          info.finish_time = now
          self.documenter.register_task_success(info, None, None)
        dtype = task.type
        nbytes = task.nbytes
        info = self.all_runnables[self.macro_tasks[key][-1]]
        info.finish_time = now
        self.documenter.register_task_success(info, dtype, nbytes)
        self.macro_tasks.pop(key) # Avoid registering two times the same activity

      elif start == 'erred' and key in self.macro_tasks:
        # It's impossible to detect what task has failed, so if this macro task
        # has many sub-tasks, all are represented as erroneous in the provenance
        # document

        now = dt.datetime.now()
        for sub_key in self.macro_tasks[key][:-1]:
          self.erred_tasks[sub_key] = self.all_tasks[sub_key]
          info = self.all_runnables[sub_key]
          info.finish_time = now
          self.documenter.register_task_failure(info, None, None, None)
        # The task is finished with an error, so register the exception
        info = self.all_runnables[self.macro_tasks[key][-1]]
        info.finish_time = now
        text = task.exception_text or ''
        blamed_task = task.exception_blame
        traceback = None
        if self.keep_traceback:
          traceback = task.traceback_text
        self.documenter.register_task_failure(info, text, traceback, blamed_task)
        self.macro_tasks.pop(key)

        # When an exception occurs, the plugin is closed before it has the chance
        # to detect the erred task and register its information. So, if the plugin
        # has already been closed, serialize the document again.
        if self.closed:
          self.documenter.serialize()
    except Exception:
      print(f'Task {key} generated an exception:\n{format_exc()}')

  async def close(self):
    self.closed = True
    self.documenter.terminate()

    try:
      self.documenter.serialize()
    except Exception as e:
      print(f'Close: {e}')

  def _track_task(self, task: TaskState) -> dict[Key, RunnableTaskInfo]:
    infos: dict[Key, RunnableTaskInfo] = {}
    pending_tasks: list[tuple[Key, Task]] = []
    infos[task.key] = RunnableTaskInfo(
      task=task, all_tasks=self.all_tasks, pending_tasks=pending_tasks
    )

    while len(pending_tasks) > 0:
      new_key, new_task = pending_tasks.pop()
      self.all_tasks[new_key] = new_task
      new_task_deps: dict[Key, Any] = { k: self.all_tasks[k] for k in new_task.dependencies }
      if not ProvTracker._is_dask_internal(new_task):
        infos[new_key] = RunnableTaskInfo(
          key=new_key, specs=new_task, group_key=task.group_key,
          all_tasks=self.all_tasks, dependencies=new_task_deps
        )
      else:
        infos.update(self._track_dask_internal(
          new_task, task.group_key, new_key, sub_tasks={})
        )
    return infos
    

  @staticmethod
  def _is_dask_internal(run_spec: Task) -> bool:
    """Checks if the given runnable task is a call to a dask internal function."""

    func = run_spec.func
    if hasattr(func, '__name__'):
      return (
        func.__name__ == '_execute_subgraph' and
        func.__module__ == 'dask._task_spec'
      )
    return False

  def _track_dask_internal(
    self, run_spec: Task, group_key: str,
    parent_key: Key,
    parent_internal_deps: dict[Key, Task | DataNode] = {},
    unique_keys: dict[Key, Key] = {},
    sub_tasks: dict[Key, Task | DataNode] = {}
  ) -> dict[Key, RunnableTaskInfo]:
    """Given a task that executes `dask._task_spec._execute_subgraph` follows
    the subgraph associated to that call and records all the information about
    tasks in that subgraph. `self.all_tasks` is kept updated. Returns a
    dictionary with the info of all analysed tasks.
    
    #### Note
    This call is recursive, but this should be safe as the maximum recursion
    level is (apparently) at most 1.
    """
    # These are all tasks that will be executed by _execute_subgraph
    inner_dsk = cast(dict[Key, Task | Alias | DataNode], run_spec.args[0])

    # Some dependencies are referenced with names in inkeys that might be
    # different from names already used before for the same resource
    inkeys = cast(tuple[Key, ...], run_spec.args[2])
    dependencies = cast(tuple[TaskRef | DataNode, ...], run_spec.args[3:])
    internal_deps: dict[Key, Any] = {}
    for key, dep in zip(inkeys, dependencies):
      if isinstance(dep, TaskRef):
        # Refs are always for already-seen tasks, so this is safe. Also, tasks
        # whose key has been changed because it was non-unique, are never seen
        # here
        if dep.key in parent_internal_deps:
          internal_deps[key] = parent_internal_deps[dep.key]
        else:
          unique_dep_key = unique_keys.get(dep.key, dep.key)
          internal_deps[key] = self.all_tasks[unique_dep_key]
      else:
        # As this value is actually ready and doesn't come from any other
        # recognizable task, just take the raw value
        internal_deps[key] = dep.value
    
    pending_tasks: list[tuple[Key, Task]] = []
    priorities = order(inner_dsk)
    infos: dict[Key, RunnableTaskInfo] = {}
    for key, node in sorted(inner_dsk.items(), key=lambda it: priorities[it[0]]):
      if str(key).startswith(('store-map', '(store-map')):
        pass
      if isinstance(node, Task):
        unique_key = make_unique_key(parent_key, key)
        unique_keys[key] = unique_key
        self.all_tasks[unique_key] = node
        sub_tasks[key] = node
        if ProvTracker._is_dask_internal(node):
          infos.update(self._track_dask_internal(
            node, group_key, unique_key, internal_deps, unique_keys, sub_tasks
          ))
        else:
          node_deps = {}
          for dep in cast(set[Key], node.dependencies):
            if dep in internal_deps:
              node_deps[dep] = internal_deps[dep]
            else:
              unique_dep_key = unique_keys.get(dep, dep)
              node_deps[dep] = self.all_tasks[unique_dep_key]
          infos[unique_key] = RunnableTaskInfo(
            key=unique_key, specs=node, group_key=group_key,
            dependencies=node_deps, all_tasks=self.all_tasks,
            unique_keys=unique_keys, pending_tasks=pending_tasks
          )
      elif isinstance(node, Alias):
        unique_key = make_unique_key(parent_key, key)
        unique_target = unique_keys.get(node.target, node.target)
        self.all_tasks[unique_key] = self.all_tasks[unique_target]
        sub_tasks[key] = self.all_tasks[unique_target]
        if isinstance(self.all_tasks[unique_key], Task):
          if unique_target in infos:
            infos[unique_key] = infos[unique_target]
          elif unique_target in self.all_runnables:
            infos[unique_key] = self.all_runnables[unique_target]
          else:
            print(f'Non existent alias to runnable task {unique_key} -> {unique_target}')
      else:
        self.documenter.register_data(node)
        self.all_tasks[node.key] = node
        sub_tasks[node.key] = node
    
    if len(pending_tasks) > 0:      
      sub_tasks.update(pending_tasks)

      while len(pending_tasks) > 0:
        new_key, new_task = pending_tasks.pop()
        self.all_tasks[new_key] = new_task
        new_task_deps: dict[Key, Any] = { k: self.all_tasks[k] for k in new_task.dependencies }
        if not ProvTracker._is_dask_internal(new_task):
          infos[new_key] = RunnableTaskInfo(
            key=new_key, specs=new_task, group_key=group_key,
            all_tasks=self.all_tasks, dependencies=new_task_deps,
            unique_keys=unique_keys, pending_tasks=pending_tasks
          )
          sub_tasks.update(pending_tasks)
        else:
          infos.update(self._track_dask_internal(
            new_task, group_key, parent_key,
            parent_internal_deps=internal_deps, unique_keys=unique_keys,
            sub_tasks=sub_tasks
          ))

      # Returns infos in the correct order
      priorities = order(sub_tasks)
      ordered_infos: dict[Key, RunnableTaskInfo] = {}
      for key, node in sorted(sub_tasks.items(), key=lambda it: priorities[it[0]]):
        if not isinstance(node, DataNode):
          unique_key = unique_keys.get(key, key)
          if unique_key not in infos:
            pass
          ordered_infos[unique_key] = infos[unique_key]
      infos = ordered_infos

    # Execute subgraph returns the output of task `outkey`, so this task is
    # treated as an alias for `outkey`
    outkey: Key = run_spec.args[1]
    outkey = unique_keys.get(outkey, outkey)
    self.all_tasks[run_spec.key] = self.all_tasks[outkey]
    return infos