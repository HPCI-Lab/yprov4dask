from concurrent.futures import ThreadPoolExecutor
from dask.order import order
from dask.task_spec import DataNode, Task, TaskRef, Alias
from dask.typing import Key
from distributed.diagnostics.plugin import SchedulerPlugin
from distributed.scheduler import Scheduler, TaskState, TaskStateState as SchedulerTaskState
from prov_tracking.documenter import Documenter
from prov_tracking.utils import make_unique_key
from prov_tracking.task_info import RunnableTaskInfo
from prov_tracking.jupyter_listener import listen

import datetime as dt
from typing import Any, cast
from traceback import format_exc
import multiprocessing as mp

class ProvTracker(SchedulerPlugin):
  """Provenance tracking plugin"""

  def __init__(self, **kwargs):
    """Additional arguments:
    - `destination: str`: folder in which the provenance document is saved. The
    file is always named `yprov4wfs.json`. Defaults to `./output`.
    - `keep_traceback: bool`: tells if the plugin should register the traceback
    of the exceptions generated by failed tasks. Defaults to `False`.
    - `rich_types: bool`: tells if datatypes of values such be richer, e.g. for
    tuples, track the type of each element instead of just saying that the value
    is a tuple. Defaults to `False`.
    - `jupyter_tracking: bool`: tells if the plugin should try to record in the
    provenance document the information about what cell of the notebook generated
    each activity. Defaults to `True`. Notice how this option creaed an additional
    thread that communicates with the Jupyter kernel.
    You can also provide all kwargs accepted by `prov.model.ProvDocument.serialize`.
    """

    name = kwargs.pop('name', __name__)
    self.keep_traceback: bool = kwargs.pop('keep_traceback', False)
    self.track_jupyter: bool = kwargs.pop('jupyter_tracking', True)
    self.documenter = Documenter(name, **kwargs)

    self.closed = False
    # Used to avoid registering multiple times the same task. A task can be put
    # multiple times in waiting state
    self.registered_tasks: set[Key] = set()
    # Info about all tasks encountered
    self.all_tasks: dict[Key, Task | DataNode] = {}
    self.all_runnables: dict[Key, RunnableTaskInfo] = {}
    # Tasks executed within _execute_subgraph are saved here under the key of
    # the task that executes _execute_subgraph. That task info are not actually
    # saved, just its children.
    self.macro_tasks: dict[Key, list[Key]] = {}
    # For each macro task keeps the dictionary used to translate non-unique keys
    # info unique ones.
    self.unique_keys: dict[Key, dict[Key, Key]] = {}
    # When Jupyter tracking is enabled, keeps the id of the last executed cell
    # in the notebook
    self.last_cell_id = None

  def start(self, scheduler: Scheduler):
    self._scheduler = scheduler
    if self.track_jupyter:
      plugin_end, listener_end = mp.Pipe(duplex=True)
      self.thread_pool = ThreadPoolExecutor(max_workers=1)
      self.jupyter_listener = self.thread_pool.submit(listen, listener_end)
      resp = plugin_end.recv()
      if resp:
        self.connection = plugin_end
      else:
        self.thread_pool.shutdown()
        self.jupyter_listener = None
        plugin_end.close()
        print("""Warning: Jupyter tracking is enabled, but the deamon could not
        be started for some reason. The tracking will proceed as if
        jupyter_tracking was set to False.""")

  def transition(
    self, key: Key, start: SchedulerTaskState, finish: SchedulerTaskState,
    *args, **kwargs
  ):
    try:
      task = self._scheduler.tasks[key]

      # Here tasks are seen in reverse dependency order, i.e. tasks with no
      # dependencies are seen before tasks which depend on them
      if start == 'waiting' and key not in self.registered_tasks:
        if finish == 'released' and not ProvTracker._has_erred_dep(task):
          # Ignore this task as one of its dependent tasks has failed and
          # hence this will no longer be executed
          return

        self.registered_tasks.add(key)
        if isinstance(task.run_spec, DataNode):
          self.all_tasks[key] = task.run_spec
          self.documenter.register_data(task.run_spec)
        elif isinstance(task.run_spec, Task):
          # Retrieve Jupyter-related info
          cell_id: int | None = self.last_cell_id
          if self.jupyter_listener is not None and self.connection.poll():
            cell_id = self.connection.recv()
            while self.connection.poll():
              cell_id = self.connection.recv()
            self.last_cell_id = cell_id
          infos = self._record_task(key, task.group_key, task.run_spec)
          self.macro_tasks[key] = []
          for sub_key, info in infos.items():
            self.macro_tasks[key].append(sub_key)
            info.jupyter_cell = cell_id
            self.all_runnables[sub_key] = info
            self.documenter.register_task(info)
        else:
          target = cast(Alias, task.run_spec).target
          self.all_tasks[key] = self.all_tasks[target]

      elif start == 'processing' and key in self.macro_tasks:
        now = dt.datetime.now()
        specs = cast(Task, task.run_spec)
        infos = self._track_task(key, task.group_key, specs)
        for sub_key, info in infos.items():
          self.macro_tasks[key].append(sub_key)
          self.all_runnables[sub_key] = info
          self.documenter.register_task(info)
        for sub_key in self.macro_tasks[key]:
          info = self.all_runnables[sub_key]
          info.start_time = now
          self.documenter.register_task_dependencies(info)

      elif start == 'memory' and key in self.macro_tasks:
        now = dt.datetime.now()
        for sub_key in self.macro_tasks[key][:-1]:
          info = self.all_runnables[sub_key]
          info.finish_time = now
          self.documenter.register_task_success(info, None, None)
        dtype = task.type
        nbytes = task.nbytes
        info = self.all_runnables[self.macro_tasks[key][-1]]
        info.finish_time = now
        self.documenter.register_task_success(info, dtype, nbytes)
        self.macro_tasks.pop(key) # Avoid registering two times the same activity

      elif (start == 'erred' or finish == 'erred') and key in self.macro_tasks:
        # It's impossible to detect what task has failed, so if this macro task
        # has many sub-tasks, all are represented as erroneous in the provenance
        # document
        now = dt.datetime.now()
        for sub_key in self.macro_tasks[key][:-1]:
          info = self.all_runnables[sub_key]
          info.finish_time = now
          self.documenter.register_task_failure(info, None, None, None)
        # The task is finished with an error, so register the exception
        info = self.all_runnables[self.macro_tasks[key][-1]]
        info.finish_time = now
        text = task.exception_text or ''
        blamed_task = task.exception_blame
        traceback = None
        if self.keep_traceback:
          traceback = task.traceback_text
        self.documenter.register_task_failure(info, text, traceback, blamed_task)

        # When an exception occurs, the plugin is closed before it has the chance
        # to detect the erred task and register its information. So, if the plugin
        # has already been closed, serialize the document again.
        if self.closed:
          self.documenter.serialize()

      # Every time a task being processed passed through the scheduler, register
      # the worker who is executing it. Multiple workers might execute the same
      # task at different times, the last one is the one that matters
      if task.processing_on is not None and key in self.macro_tasks:
        processing_on = f'{task.processing_on.address}/{task.processing_on.name}'
        for sub_key in self.macro_tasks[key]:
          info = self.all_runnables[sub_key]
          info.processed_on = processing_on
          self.documenter.register_task_worker(info)
    except Exception:
      print(f'Task {key} generated an exception:\n{format_exc()}')

  async def close(self):
    self.closed = True
    if self.connection is not None and self.connection.closed:
      self.thread_pool.shutdown(wait=False)
      self.connection.send(True)
      self.connection.close()
      self.jupyter_listener = None

    try:
      self.documenter.serialize()
    except Exception as e:
      print(f'Close: {e}')

  def _record_task(self, key: Key, group_key: str, specs: Task) -> dict[Key, RunnableTaskInfo]:
    """Records the existance of this task and all its subtasks, if any, in both
    the provenance document and the plugin itself. Returns a dictionary in which
    each item pairs info of a task to the unique key for that task.
    
    The only internal structures that are directly modified by this method are
    `self.all_tasks` and self.unique_keys, the latter only for the dictionary
    associated with `key`."""

    infos = {}
    task_unique_keys: dict[Key, Key] = { key: key }
    if ProvTracker._is_expandable_task(specs):
      infos.update(self._record_expandable_task(
        key, group_key, specs, task_unique_keys
      ))
    else:
      self.all_tasks[key] = specs
      infos[key] = RunnableTaskInfo(key, group_key, specs)
    self.unique_keys[key] = task_unique_keys
    return infos

  def _record_expandable_task(
    self, key: Key, group_key: Key, specs: Task, unique_keys: dict[Key, Key]
  ) -> dict[Key, RunnableTaskInfo]:
    """If the task can be expanded, i.e. this task embedes other subtasks that
    must be recorded, find all such tasks and returns their infos paired with
    their unique key."""

    infos: dict[Key, RunnableTaskInfo] = {}
    inner_dsk = cast(dict[Key, Task | Alias | DataNode], specs.args[0])
    outkey: Key = specs.args[1]
    refkey = unique_keys[key] # Key used as reference to make subkeys unique
    
    priorities = order(inner_dsk)
    for key, node in sorted(inner_dsk.items(), key=lambda it: priorities[it[0]]):
      if isinstance(node, DataNode):
        self.documenter.register_data(node)
        self.all_tasks[node.key] = node
      elif isinstance(node, Alias):
        unique_key = make_unique_key(refkey, key)
        unique_keys[key] = unique_key
        unique_target = unique_keys.get(node.target, node.target)
        self.all_tasks[unique_key] = self.all_tasks[unique_target]
        if isinstance(self.all_tasks[unique_key], Task):
          if unique_target in infos:
            infos[unique_key] = infos[unique_target]
          elif unique_target in self.all_runnables:
            infos[unique_key] = self.all_runnables[unique_target]
          else:
            print(f'Non existent alias to runnable task {unique_key} -> {unique_target}')
      else:
        unique_key = make_unique_key(refkey, key)
        unique_keys[key] = unique_key
        if ProvTracker._is_expandable_task(node):
          infos.update(self._record_expandable_task(
            key=unique_key, group_key=group_key, specs=node,
            unique_keys=unique_keys
          ))
        else:
          self.all_tasks[unique_key] = node
          infos[unique_key] = RunnableTaskInfo(unique_key, group_key, node)

    outkey = unique_keys.get(outkey, outkey)
    self.all_tasks[refkey] = self.all_tasks[outkey]
    return infos

  def _track_task(self, key: Key, group_key: Key, specs: Task) -> dict[Key, RunnableTaskInfo]:
    """Given a task that has already been recorded, tracks all its dependencies.
    It might happen that among the dependencies (actually among the arguments to
    functions) there are tasks that must be created by the plugin in order to be
    recorded. If any such new tasks exist, their keys are returned toghether
    with their info."""

    new_infos = {}
    pending_tasks: list[tuple[Key, Task]] = []
    task_unique_keys = self.unique_keys[key]
    if ProvTracker._is_expandable_task(specs):
      new_infos.update(self._track_expandable_task(
        specs=specs, group_key=group_key, unique_keys=task_unique_keys
      ))
    else:
      info = self.all_runnables[key]
      dependencies: dict[Key, Task | DataNode | Alias] = {}
      for dep_key in cast(set[Key], specs.dependencies):
        unique_key = task_unique_keys.get(dep_key, dep_key)
        dependencies[dep_key] = self.all_tasks[unique_key]
      info.record_dependencies(
        dependencies=dependencies, all_tasks=self.all_tasks,
        unique_keys=task_unique_keys, pending_tasks=pending_tasks
      )
      if len(pending_tasks) > 0:
        while len(pending_tasks) > 0:
          new_key, new_task = pending_tasks.pop()
          task_unique_keys[new_key] = new_key
          new_task_deps: dict[Key, Any] = {}
          for dep_key in cast(set[Key], new_task.dependencies):
            unique_key = task_unique_keys.get(dep_key, dep_key)
            new_task_deps[dep_key] = self.all_tasks[unique_key]
          if ProvTracker._is_expandable_task(new_task):
            new_infos.update(self._track_expandable_task(
              group_key=group_key, specs=new_task, unique_keys=task_unique_keys,
              parent_internal_deps={}
            ))
          else:
            self.all_tasks[new_key] = new_task
            # Record the new task
            info = RunnableTaskInfo(
              key=new_key, specs=new_task, group_key=group_key
            )
            info.record_dependencies(
              dependencies=new_task_deps, all_tasks=self.all_tasks,
              unique_keys=task_unique_keys, pending_tasks=pending_tasks
            )
            new_infos[new_key] = info
    return new_infos
    
  @staticmethod
  def _is_expandable_task(specs: Task) -> bool:
    """Checks if the given runnable task expands into multiple tasks."""

    func = specs.func
    if hasattr(func, '__name__'):
      return (
        func.__name__ == '_execute_subgraph' and
        func.__module__ == 'dask._task_spec'
      )
    return False

  def _track_expandable_task(
    self, group_key: Key, specs: Task,
    unique_keys: dict[Key, Key] = {},
    parent_internal_deps: dict[Key, Task | DataNode] = {},
  ) -> dict[Key, RunnableTaskInfo]:
    """Tracks dependecies for an expandable tasks, i.e. tracks the dependecies
    of all tasks embedded in an expandable task."""

    # These are all tasks that will be executed by _execute_subgraph
    inner_dsk = cast(dict[Key, Task | Alias | DataNode], specs.args[0])

    # Some dependencies are referenced with names in inkeys that might be
    # different from names already used before for the same resource
    inkeys = cast(tuple[Key, ...], specs.args[2])
    dependencies = cast(tuple[TaskRef | DataNode, ...], specs.args[3:])
    internal_deps: dict[Key, Any] = {}
    for key, dep in zip(inkeys, dependencies):
      if isinstance(dep, TaskRef):
        # Refs are always for already-seen tasks, so this is safe. Also, tasks
        # whose key has been changed because it was non-unique, are never seen
        # here
        if dep.key in parent_internal_deps:
          internal_deps[key] = parent_internal_deps[dep.key]
        else:
          unique_dep_key = unique_keys.get(dep.key, dep.key)
          internal_deps[key] = self.all_tasks[unique_dep_key]
      else:
        # As this value is actually ready and doesn't come from any other
        # recognizable task, just take the raw value
        internal_deps[key] = dep.value
    
    pending_tasks: list[tuple[Key, Task]] = []
    priorities = order(inner_dsk)
    infos: dict[Key, RunnableTaskInfo] = {}
    for key, node in sorted(inner_dsk.items(), key=lambda it: priorities[it[0]]):
      # DataNode and Alias cases happens only if node comes from a pending task
      # identified by RunnableTaskInfo.record_dependencies, and only if that
      # node is expandable
      if isinstance(node, DataNode):
        if node.key not in self.all_tasks:
          self.documenter.register_data(node)
          self.all_tasks[node.key] = node
      elif isinstance(node, Alias):
        if node.key not in self.all_tasks:
          unique_key = unique_keys[node.key]
          unique_target = unique_keys.get(node.target, node.target)
          target = self.all_tasks[unique_target]
          if isinstance(target, Task):
            if unique_target in infos:
              infos[unique_key] = infos[unique_target]
            elif unique_target in self.all_runnables:
              infos[unique_key] = self.all_runnables[unique_target]
            else:
              print(f'Non existent alias to runnable task {unique_key} -> {unique_target}')
      else:
        unique_key = unique_keys[key]
        if ProvTracker._is_expandable_task(node):
          infos.update(self._track_expandable_task(
            group_key=group_key, specs=node, unique_keys=unique_keys,
            parent_internal_deps=internal_deps
          ))
        else:
          node_deps = {}
          for dep in cast(set[Key], node.dependencies):
            if dep in internal_deps:
              node_deps[dep] = internal_deps[dep]
            else:
              unique_dep_key = unique_keys.get(dep, dep)
              node_deps[dep] = self.all_tasks[unique_dep_key]
          self.all_runnables[unique_key].record_dependencies(
            dependencies=node_deps, all_tasks=self.all_tasks,
            unique_keys=unique_keys, pending_tasks=pending_tasks
          )
    
    # Here, any pending task identified by RunnableTaskInfo.record_dependencies
    # is recorded into the prov document and the plugin
    if len(pending_tasks) > 0:      
      while len(pending_tasks) > 0:
        new_key, new_task = pending_tasks.pop()
        unique_keys[new_key] = new_key
        new_task_deps: dict[Key, Any] = {}
        for dep_key in cast(set[Key], new_task.dependencies):
          unique_key = unique_keys.get(dep_key, dep_key)
          new_task_deps[dep_key] = self.all_tasks[unique_key]
        if ProvTracker._is_expandable_task(new_task):
          infos.update(self._track_expandable_task(
            group_key=group_key, specs=new_task, unique_keys=unique_keys,
            parent_internal_deps=internal_deps
          ))
        else:
          self.all_tasks[new_key] = new_task
          # Record the new task
          info = RunnableTaskInfo(
            key=new_key, specs=new_task, group_key=group_key
          )
          info.record_dependencies(
            dependencies=new_task_deps, all_tasks=self.all_tasks,
            unique_keys=unique_keys, pending_tasks=pending_tasks
          )
          infos[new_key] = info
    return infos

  @staticmethod
  def _has_erred_dep(task: TaskState) -> bool:
    return any((dep.state == 'erred') for dep in task.dependencies)